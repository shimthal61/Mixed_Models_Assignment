---
title: "Assignment 1"
author: '10179889'
date: "09/02/2022"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float: yes
    font:family: Lato
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

For this assignment, we have been provided with two different datasets. Using the knowledge we have gained from the workshops combined with further reading and wider resources, I will:

-   Wrangle and tidy the data
-   Summarise the data
-   Visualise the data
-   Build and interpret the appropriate models

## Packages

First, let's load in our packages using the `library()` function:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(visdat)
library(lme4)
library(lmerTest)
library(performance)
library(fitdistrplus)
library(emmeans)
library(ggthemes)
library(showtext)
library(buildmer)
```

The `{tidyverse}` package contains a collection of open source R packages that gives us access to a variety of useful functions and commands for data wrangling and visualisation. We can use the `{visdat}` package to visualize our data. This makes it easy to identify problems, such as missing data. `{lme4}` provides us the functions for fitting and analyzing the mixed models we'll be building later, while `{lmerTest}` provides us with the *p*-values for our models. The `{performance}` lets us visually check our models across various assumptions, such as normality of residuals. We can use the `descdist()` function within `{fitdistrplus}` to identify our data distribution along a Cullen and Frey graph. The `{emmeans}` package provides us with the means to conduct pairwise comparisons on our models. One of the most useful packages I've come across is `buildmer()`.After feeding it our maximal mixed model, it will fit all different parameters of the model until it finds the model with maximal *feasible* model. This automated process not only saves time, but ensures that the final model contains as many of our parameters as possible. The `{showtext}` package enables us to load in and use over 1291 fonts from [Google Fonts](https://fonts.google.com/) for our data visualisations, whilst `{ggthemes}` contains extra themes for our plots. 

To be able to dictate the font family later, we first need to download a font from [Google Fonts](https://fonts.google.com/). I decided to pick the *Lato* Font, as its larger character height lends itself to easier readability at small sizes. Within the `font_add()` command, we dictate what name to assign to our font as well as the pathway to the file. Finally, we tell `{showtext}` to automatically render the text everytime it's called on.

```{r}
font_add("lato", regular = "Lato-regular.ttf", bold = "Lato-Bold.ttf")
showtext_auto()
```

# Question 1

Let's read in our data and look over the information for question 1:

> <font size = "3"> We have 24 participants in a repeated measures design where we are interested in the effect of context on response time. Our experimental factor (Context) has 3 levels (Neutral vs. Negative vs. Positive). The time it took for participants to respond, which was taken as a measure of reaction time in milliseconds, served as the dependent variable. There were 24 items, and each item appeared in each of the three contexts. </font>

Let's read in our data using the `read_csv()` function - we've organised our files using a .Rproj type to ensure reproducibility. We can glimpse the first 6 rows of our data using the `head()` function:

```{r, message = FALSE}
q1_data_raw <- read_csv("assignment1_data1.csv")
head(q1_data_raw)
```

## Data Wrangling

Using the `str()` function, we can display the structure of our data.

```{r, message = FALSE}
str(q1_data_raw)
```

As it currently stands, our `subj`, `item`, and `condition` columns are not coded as factors, but simply as characters (`chr`). We can use the `transmute()` function to replace these variables with their factorised equivalents. Within this argument, we can also change the column names to make them more meaningful. We'll pass this on to a new variable, which we'll call `q1_data_tidied`:

```{r}
q1_data_tidied <- q1_data_raw %>% 
  transmute(Subject = factor(subj),
         Item = factor(item),
         Context = factor(condition),
         Response_Time = DV)
head(q1_data_tidied)
```

This is better - Our columns are coded correctly and are labelled meaningfully. Moreover, all data is currently in `long` format; every row corresponds to one observation, which is key for building our models later. Using the `vis_miss()` function, we are provided with an at-a-glance plot of any missing data inside our dataframe.

```{r}
vis_miss(q1_data_tidied)
```

All our data is present - let's generate some summary statistics to get an understanding of any effects in our data.

## Data Summarising

Let's create a tibble for the summary statistics of our dataset. We'll first utilise the `group_by()` function to gather our Context variable, then `summarise()` to create a new table consisting of Response Time mean and standard deviation. The data can be arranged from the fastest mean Response Time to the slowest using the `arrange()` function:

```{r}
q1_data_tidied %>% 
  group_by(Context) %>% 
  summarise(mean_RT = mean(Response_Time), sd_RT = sd(Response_Time)) %>%
  arrange(mean_RT)
```

At a glance, it looks as though participants who were presented with items in a negative context had faster response times than those who were presented with items in neutral or positive contexts. At this stage, we can create some data visualisations to better present our results.

## Data Visualisations

We first specify within `ggplot()` what data we want included in our visualisation. We then plot the raw data points using the `geom_point` function, as well as a violin plot to display our data density. If we were to use the `geom_jitter()` function to jitter our data, every time we execute the code, the points will jitter to a different position. This implies that our code is not reproducible, because we don't know the seed that *R* will use to generate the sequence. Inside `geom_point()`, however, we are able to set `seed = 42` within the `position = position_jitter` argument to ensure that the output will be identical every time it is run. I set the seed to 42 for its reference to Hitch Hikers Guide to the Galaxy...

I removed the legend using `guides(colour = 'none')` as it provides no additional information, as have added some summary data in the form of the Mean and Confidence Intervals using the `stat_summary()` function. We have dictated the y-axis breaks and limits within the `scale_y_continuous()` function so that our the range fully incorporates our data, and defined a . Within the `theme()` function, I've changed the plot font to `lato`, made appropriate the text size, and moved the axis and title text away from the graph within the `margin` argument. Finally, we have flipped the axes around as I believe that it aids overall interpretation of the results.

```{r, out.width="200%"}
q1_data_tidied %>% 
    ggplot(aes(x = Context, y = Response_Time, colour = Context)) +
    geom_violin(width = 0.5, lwd = 0.7) +
    geom_point(alpha = 0.2, position = position_jitter(width = 0.08, seed = 42)) +
    guides(colour = 'none') +
    stat_summary(fun.data = 'mean_cl_boot', colour = 'black') +
    labs(y = "Response Time (ms. )",
         title = "Effect of Item Context on Reaction Time") +
    scale_y_continuous(breaks = seq(0, 6000, by = 1000),
                       limits = c(0, 6000)) +
    theme_minimal() +
    theme(
      text = element_text(family = "lato", size = 28),
      plot.title = element_text(size = 38, hjust = 0.5, margin = margin(b = 20), face = "bold"),
      axis.title.x = element_text(size = 38, margin = margin(t = 20)),
      axis.title.y = element_text(size = 38, margin = margin(r = 20))) +
    coord_flip()
```

<p>&nbsp;</p>

Looking at our graph, it appears that participants did indeed respond quicker to items in `Negative` Contexts. It also looks items presented in `Positive` contexts elicited slower responses, with `Neutral` contexts sitting in between the two. Interestingly, we can see from the long tails in the `Positive` and `Negative` contexts that we have a couple of observations that had significantly slower response times- we'll come back to this later. This is an example of why it's important to always visualise data in combination with summary statistics.

We can now build our linear mixed model to see whether the levels in our condition are statistically different from each other. 

## Building our Linear Mixed-Model

### Setting up our contrasts

Since we're using dummy (treatment) coding for the levels our of experimental factor, let's have a look at how our contrasts have been coded using the `contrasts()` function.

```{r}
contrasts(q1_data_tidied$Context)
```

As it stands, `Negative` contexts are currently our reference level. We want it so that `Neutral` contexts becomes our reference level. This means that when we build our model, the intercept will correspond to the mean of our `Neutral` group. Changing the structure will make it easier to interpret the model parameter estimates (although it is not entirely necessary to do this when we have only one independent variable). We can do this using the `fct_relevel` argument within the `mutate()` function.

```{r}
q1_data_tidied <- q1_data_tidied %>% 
  mutate(Context = fct_relevel(Context,
                               c("Neutral", "Negative", "Positive")))
contrasts(q1_data_tidied$Context)
```

OK, this is what we want - `Neutral` contexts are coded as `0, 0`. Let's now build our mixed model.

### Creating our Maximal Model

Let's first create our maximal model using the `lmer()` function. We're asking for `Response_Time` to be predicted by fixed effect, `Context`, plus the random effect of `Item`, plus the random effect of `Subject`. This means that we're modelling an individual baseline response time for each of our items and random effects. Within our random effects, our model takes into consideration that the difference between the three levels of our experimental factor might vary between subjects and items. Finally, we're passing this all on to a variable called `q1_model_max`.

```{r}
q1_model_max <- lmer(Response_Time ~ Context + 
                    (1 + Context | Subject) +
                    (1 + Context | Item),
                  q1_data_tidied)
```

When we build this model, we're outputted with the error message `?isSingular` - we have obtained a singular fit. This indicates that the model that we've built, when considering the number of parameters we're trying to estimate, is more complex than our dataset will allow us to build. In other words, the random effect structure is too complex relative to the richness (or lack of) of our dataset. We can choose to ignore the warning, since it is not a fatal error, especially if we had a strong theoretical reason to model all of the terms in the random effect structure. Alternatively, we could gradually simplify the random effects structure until we resolve the error. The benefit of this approach is that it leads to a more parsimonious model that is not over-fitted.

### Using `buildmer()` to Fit our Model

In order to build our model with the correct fit, we would normally need to simplify our random effect structure until we reach a model which fits our data-set. This would mean continually building models in a backwards direction until we found a model which didn't spit out the `?isSingular` warning message. As well as being time-consuming, this method is not guaranteed to produce the Maximal Feasible Model (MFM).  

We can use the `{buildmer}` package to remedy this dilemma. The package consists of a number of functions designed to fit specific types of models. By using the `lmer()` function from `{lme4}`, the  `buildmer()` function fits our mixed-effect model to find the MFM. The good thing about this package is that it uses the exact syntax as building a model in `{lme4}`, where the formula we provide is the maximal model we can possibly build. Within the `buildmerControl()` argument, we set the `direction` parameter to `'order'`. This means that `buildmer()` will start out with an empty model and keep adding terms to it until convergence can no longer be achieved. `buildmer()` adds terms in order of their contribution to a certain criterion, resulting in the most important random slopes being included. Finally, we use the `quiet = TRUE` argument to hide the fitting progress messages. If this was left on, then our html doc would be bogged down will potential models that `buildmer()` tried to fit! We can view the the formula for our MFM usng the `formula()` function. 

```{r}
q1_buildmer_model <- buildmer(Response_Time ~ Context + 
                    (1 + Context | Subject) +
                    (1 + Context | Item),
                  q1_data_tidied,
                  buildmerControl = buildmerControl(direction = 'order', quiet = TRUE))
formula(q1_buildmer_model)
```

Our MFM has response time predicted by context, whilst modelling the random effects of subjects and items, where each subject and item has its own baseline. However, because we've dropped the random slopes, we're assuming that the difference between the three levels of our experimental condition are the same for all subjects and items. It is worth considering that this increases the likelihood of a type 1 error occurring, leading us to think that we have an effect when one is not actually present.

For a very helpful demonstration on using `buildmer` to automatically find and compare maximal mixed models, I recommend this vignette by Cesko C. Voeten, 2021. Click on the image below to access it. 

<center>

[![Link to Cesko C. Voeten vignette](buildmer.png){width=75%}](https://cran.r-project.org/web/packages/buildmer/vignettes/buildmer.html)

</center>

Before we continue, it is worth noting that the model that `buildmer` has fit is currently defined as being of `buildmer` class. This means that the model is incompatible with future functions - we assess assumptions or investigate interactions. Luckily, we know the formula for this model, which we can map onto a new variable of `lmer` class:

```{r}
q1_MFM_model <- lmer(Response_Time ~ Context + 
                   (1 | Subject) +
                   (1 | Item),
                  q1_data_tidied)
```

### Checking Normality Assumptions

Before we move on, we should check to see whether our model adheres to normality assumptions. To do this, we can use the `check_model()` function found within the `{performance}` package. This will output a number of diagnostic plots and allow us to quickly tell whether our mixed model seems to be consistent with the assumptions behind mixed models.

```{r}
check_model(q1_MFM_model)
```

This is interesting - our model seems to have some deviation in its normality of residuals, as well as a few influential observations. The `{performance}` package contains a variety of functions which allows us to check normality assumptions a little more closely. The `check_normality()` function uses the Shapiro-Wilks test for normality, where a *p*-value less than or equal to .05 states with 95% confidence that the data does not fit the normal distribution. However, it is worth noting that this formal test *almost always* yields significant results for the distribution of residuals. 

```{r}
check_normality(q1_MFM_model)
```

With a *p* < .001, it is clear that we have skewed residuals. We can use the package `{fitdistrplus}` to further investigate the data distribution. The function `descdist()` computes descriptive parameters of our distribution and provides a kewness-kurtosis plot on a Cullen and Frey graph.

```{r}
descdist(q1_data_tidied$Response_Time)
```

We have lots of different types of data distributions plotted on the Cullen and Frey graph, with our observed data indicated by the blue circle. It appears as though our dataset is quite far away from the theoretical normal distribution. We can visualise the distribution using a histogram with density along the y-axis, and overlay the plot with a translucent density plot.

```{r, out.width="200%"}
q1_data_tidied  %>% 
  ggplot(aes(x = Response_Time)) +
  geom_histogram(aes(y =..density..),
                 binwidth = 100,
                 colour = "black", fill = "black",
                 alpha = 0.5) +
  geom_density(alpha = .5, fill = "#FF6666") +
  theme_minimal() +
  labs(x = "Response Time (ms. )",
       y = "Density",
       title = "Histogram and Density Plot of Response Time",
       subtitle = "Our data seems to be more closely\naligned with the Gamma distribution") +
  theme(plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 10), line = 0.5, face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        plot.subtitle = element_text(line = 0.5, hjust = 0.06, margin = margin(b = 10)),
        text = element_text(family = "lato", size = 25))
```

It looks as though we have something similar to the Gamma distribution, which is typical of reaction time data. There is a point below which we don't have many reaction time measures because of physiological constraints. We also have a number of observations of people who have slower reaction times, which produces the gradual gradient of the tail. 

We can also check to see where our influential observations lie using the `check_outliers()` function combined with a plot of the observations. We've put the `results` variable in parentheses so that it runs automatically. 

```{r}
(results <- check_outliers(q1_MFM_model))
plot(results, type = "bars")
```

We are told that we have two influential observations. We can do a simple search for them using `filter()` to gather response times larger than 3600. 

```{r}
q1_data_tidied %>% 
  filter(Response_Time > 3600)
```

```{r}
buildmer_test <- buildmer(Response_Time ~ Context +
                            (1 + Context | Subject) +
                            (1 + Context | Item),
                          data = q1_data_tidied,
                          family = Gamma,
                          nAGQ = 0,
                          buildmerControl = buildmerControl(direction = 'order',
                                                            quiet = TRUE))
formula(buildmer_test)
```


We have two response times that are far above 3600ms. We can approach this new information in a few different ways. We could choose to ignore the outliers, as they can be informative about the subject-area. Alternatively, we could remove the outliers from the dataset entirely. When deciding what approach to take, I refereed back to the George Box quote:

`"All models are wrong, but some are useful."`

Since there is no theoretically *correct* way to progress, I believe that the best approach is to build two models - one with and one without the outliers. If both results yield a similar narrative, then we can be confident that the effect exists regardless of the influential observations. Let's create a new dataset without the outliers.

```{r}
q1_data_removed <- q1_data_tidied %>% 
  filter(Response_Time < 3600)
head(q1_data_removed)
```

We can now build our MFM using `buildmer()` and map the resulting formula onto a `lmer`.

```{r}
q1_removed_buildmer <- buildmer(Response_Time ~ Context +
                                 (1 + Context | Subject) +
                                 (1 + Context | Item),
                               data = q1_data_removed,
                               buildmerControl = buildmerControl(direction = 'order',
                                                                 quiet = TRUE))
formula(q1_removed_buildmer)
```

```{r}
q1_MFM_removed <- lmer(Response_Time ~ 1 + Context +
                         (1 | Subject) +
                         (1 | Item),
                       data = q1_data_removed)
```

## Interpreting our results

Now that our two different models have been built, we can get the output of these models using the `summary()` function:  

**Model with outliers**

```{r}
summary(q1_MFM_model)
```

**Model without outliers**

```{r}
summary(q1_MFM_removed)
```

Our two models have given us very similar outputs. More of the random variability can be attributed to our subjects as oppose the the items. We are particularly interested in our fixed effects. Due to how we previously set up our contrasts, the `Intercept` corresponds to our `Neutral` contexts level. Crucially, in both models, `Negative` contexts differed significantly from `Neutral` contexts. Both models also told us that `Positive` contexts did not differ signficantly from `Neutral` contexts.

However, only part of the story has emerged so far. The problem with this kind of dummy coding is that there has not yet been a comparison of `Positive` contexts against `Negative` contexts. 

### Likelihood Ratio Test

Let's now compare our experimental model to a model which has dropped the fixed effect of context. First, we need to build this latter model, which we'll call our null model. We'll use the likelihood ratio test (LRT) to assess whether the models are statistically different from each other. It is worth noting that models can only be compared to each other if they are nested - it is essential that our null model is a subset of our full factorial model, and contains the same random effects. This means that we are unable to use the LRT to assess the differences between our model without the outliers to our null model, since they were fitted to a different dataset. This shouldn't be too much of an issue considering that our two models told very much the same story. 

```{r}
q1_model_null <- lmer(Response_Time ~ 
                        (1 | Subject) +
                        (1 | Item),
                    data = q1_data_tidied)
```

We can them compare our two models with the LRT within the `anova()` function:

```{r, warning=FALSE, message=FALSE}
anova(q1_MFM_model, q1_model_null)
```

If we have a look at our *p*-value, we can see that our two models differ significantly from each other (*p* < .001. Moreover, by looking at our deviance scores, the residual sums of squares is less for our model with our fixed effects. This tells us that our experimental manipulation seems to be making a difference. As expected, our AIC, BIC, and deviance scores are all lower in our model which contains our experimental factor. This all tells us that our experimental manipulation seems to be making a difference. 

### Pairwise Comparisons

Following this, we run to run pairwise comparisons where we control for the familywise error rate. We're going to use the `emmenas()` function from the `{emmeans}` package to run our multiple comparisons:

```{r}
emmeans(q1_MFM_model, pairwise ~ Context)
```

In this output, we get the estimates for our three experimental conditions, as well as our pairwise comparisons, which compares our condition with each other condition in all possible combinations. By default, the `{emmeans}` function uses the Tukey corrected comparisons method. If we correct for multiple comparisons, we see that the only pairwise comparison that was not carried out when using the `summary()` function is significant - `Negative` vs `Positive` conditions. The `Neutral` vs `Negative` conditions, which was significant in the model estimates earlier, is no longer significant when taking into account multiple comparisons.

## Conclusion

Post hoc comparisons applying Tukey correction confirmed that words appearing in Negative contexts resulted in significantly faster response times than words appearing in Positive contexts (*p* = .002), but not Neutral contexts (*p* = .264). The difference in response times given for words in Neutral contexts and Positive Contexts was not statistically significant (*p* = .089).

By building linear mixed models, we were able to model the individual variation due to participants and items, and build up a model which was able to take on board all of the information in our dataset. By working over the raw data points themselves, the linear mixed model approach used a lot more information that is in the dataset.

# Question 2

> <font size = "3"> In a 2 x 2 repeated measures experiment, participants had to respond to a face that depicted either "Anger" or "Fear". Each face was presented after a Story Vignette that described either an angry or fearful situation. There were 32 participants and 32 vignettes. Two independent variables were manipulated within-participants in a fully factorial design; story emotion with two levels (Anger and Fear) and Face Expression with two levels (Anger and Fear). The time it took for participants to respond to the face, which was taken as a measure of reaction time, served as the dependent variable. </font>

Let's read in our data using the `{tidyverse}` function `read_csv()`:

```{r}
q2_data_raw <- read_csv("assignment1_data2.csv")
head(q2_data_raw)
```

## Data Wrangling

Like the previous question, we need to code all our columns (bar our DV) as factors. The column titles are labelled meaningfully enough, so no need to change them.

```{r}
q2_data_tidied <- q2_data_raw %>% 
  mutate(Subject = factor(Subject),
            Vignette = factor(Vignette),
            StoryEmotion = factor(StoryEmotion),
            FaceExpression = factor(FaceExpression))
head(q2_data_tidied)
```

This is much better. We can use the `str()` function to explore our tibble in a bit more detail:

```{r}
str(q2_data_tidied)
```

We can see that `StoryEmotion` and `FaceExpression` now have their two levels, `Anger` and `Fear`, coded in. Let's quickly check for missing data using the `vis_miss()` function.

```{r}
vis_miss(q2_data_tidied)
```

All our data is present. Let's move on to generating summary statistics

## Data Summarising

Let's generate some descriptive statistics to get an idea of what's going on. We need to group both of our independent variables using the `group_by()` function, then ask *R* to work out the mean and standard deviation of reaction time within the `summarise()` function. Like before, we can arrange the mean values in ascending order to see which group had the fastest reaction times. I've mapped the outcome onto a new variable for later use in an interaction plot.

```{r}
q2_descriptives <- q2_data_tidied %>% 
  group_by(StoryEmotion, FaceExpression) %>% 
  summarise(mean = mean(RT), sd = sd(RT)) %>% 
  arrange(mean)
head(q2_descriptives)
```

At face value, it looks like participants were quickest at identifying the facial expressions when the story emotion preceding it was consistent with the face emotion. Let's generate a visualisation to get a clearer idea.

## Data Visualisation

In this visualisation, I have plotted our `StoryEmotion` condition along the y-axis and labelled them accordingly using the `scale_x_discrete()` function. I have also used the legend and an appropriate colour scheme to indicate which data represent our `FaceExpression` condition.  

```{r, out.width="200%"}
q2_data_tidied %>% 
  ggplot(aes(x = StoryEmotion:FaceExpression, y = RT, colour = FaceExpression)) +
  geom_violin() +
  geom_point(alpha = 0.2, position = position_jitter(width = 0.1, seed = 42)) +
  stat_summary(fun.data = 'mean_cl_boot', colour = 'black') +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 10000, by = 1250),
                     limits = c(0, 10000)) +
  scale_x_discrete(labels = c("Anger:Anger" = "Anger",
                              "Anger:Fear" = "Anger",
                              "Fear:Anger" = "Fear",
                              "Fear:Fear" = "Fear")) +
  labs(x = "Story Emotion",
       y = "Reaction Time (ms. )",
       title = "Examining the Effect of Story Emotion\n and Face Expression on Reaction Time") +
  theme(plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 25), line = 0.5, face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        text = element_text(family = "lato", size = 25)) +
  coord_flip()
```

<p>&nbsp;</p>

We can also build an interaction plot to get an idea of any interactions that may be present. First, we'll bring the legend closer to the lines by creating a new dataset that includes the y-values of the points where both lines end. We also need the labels that will be at the end of the lines. For this we use the function `case_when()`. We can also use the `\n` expression to dictate that we want the word 'Expressions` to be on a new line.

```{r}
labels <- q2_descriptives %>% 
  filter(StoryEmotion == "Fear") %>% 
  mutate(label = case_when(FaceExpression == "Anger" ~ "Angry Face\nExpression",
                           FaceExpression == "Fear" ~ "Fearful Face\nExpressions"))
head(labels)
```

Using the `descriptive_stats` dataset we created earlier, we can add the geom_text to our interaction plot and pass our dataset `labels` to the geom. Among the aesthetics, `geom_text()` includes the aesthetic label, which stands for the text we will add to the visualization. We've also scaled down the y-axis using the `scale_y_continuous()` function to visually increase the distance between the means, aiding interpretation. From the package `{ggtheme}`, we've added `theme_hc` to our plot. This removes the panel grid from our x-axis, as it doesn't add anything to our graph.  

```{r, out.width="200%"}
q2_descriptives %>% 
  ggplot(aes(x = StoryEmotion, y = mean)) +
  geom_line(aes(linetype = FaceExpression, group = FaceExpression, colour = FaceExpression), 
            alpha =0.8, size = 1.2, show.legend = FALSE) +
  geom_point(size = 2.6, aes(colour = FaceExpression), shape = 15) +
  geom_text(size = 8, aes(label = label,
                          colour = FaceExpression),
            data = labels,
            nudge_x = 0.19,
            nudge_y = 75,
            lineheight = 0.5) +
  guides(colour = 'none') +
  scale_y_continuous(breaks = seq(1800, 2800, by = 200),
                     limits = c(1800, 2800)) +
  labs(x = "Story Emotion",
       y = "Reaction Time (ms. )",
       title = "Examining the Interaction Between\nStory Emotion and Face Expression") +
  theme_hc() +
  theme(plot.title = element_text(size = 40, hjust = 0.5, line = 0.5, margin = margin(b = 25), face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        text = element_text(family = "lato", size = 25))
```


<p>&nbsp;</p>

We can see here that we have a crossover interaction as the polarity of the difference flips. The graph shows that Reaction Time was faster for Angry Face Expressions when the preceding story was also angry. Likewise, Reaction Time was faster for Fearful Face Expressions when the preceding story was also Fearful. Due to our crossover interaction, it is unlikely that we'll have significant main effects of *both* `StoryEmotion` and `FaceExpression` variables, but a significant interaction effect is likely.

## Building our Mixed-Model

### Setting up our Contrasts

*R* assigns Treatment Contrasts to factors and orders their levels alphabetically. This default mapping will only be correct for a given dataset if the levels' alphabetical ordering matches our desired contrast coding. This is problematic for models with interactions, as it makes it harder for us to interpret our parameter estimates, and critically, can result in misinterpretation of main vs simple effects. Rather than using the treatment coding we get by default, we're going to use sum (sometimes referred to as deviation) contrast coding, where we want to end up with a mixed-model with the intercept equal to the grand mean of our experimental conditions. This makes it much easier to estimate the parameters in our model, and also ensures that we're interpreting main effects instead of simple effects. 

In the code chunk below, we're setting up the contrasts separately for each of our experimental factors. We're specifying within the `matrix()` argument that we want the intercept to correspond to the average of our four experimental conditions. 

```{r}
contrasts(q2_data_tidied$StoryEmotion) <- matrix(c(.5, -.5))
contrasts(q2_data_tidied$FaceExpression) <- matrix(c(.5, -.5))
```

### Building our Maximal Feasible Model

Let's first create a maximal model where we attempt to model the most complex random effects structure. We're attempting to model our random effects such that the main effects *and* interaction effects differ for each of our participants and each vignettes. 

```{r}
q2_maximal_model <- lmer(RT ~ StoryEmotion * FaceExpression +
                   (1 + StoryEmotion * FaceExpression | Subject) +
                   (1 + StoryEmotion * FaceExpression | Vignette),
                 data = q2_data_tidied)
```

When we try to build our maximal model, we receive a warning message telling us that it was unable to converge on a solution of a model that fits our dataset. In other words, our model is too complicated in relation to our data-set. Like in the previous question, we can use the infinitely useful package `{buildmer}` to find out MFM for us. Let's paste our maximal formula into the `buildmer()` function. Again, we need to specify that we want our model built from the ground up using the `direction = 'order'` argument. We've also hidden the fitting process using `quiet = TRUE`.

```{r, message=FALSE}
q2_buildmer_model <- buildmer(RT ~ StoryEmotion * FaceExpression +
                          (1 + StoryEmotion * FaceExpression | Subject) +
                          (1 + StoryEmotion * FaceExpression | Vignette),
                        data = q2_data_tidied,
                        buildmerControl = buildmerControl(direction = 'order', quiet = TRUE))
formula(q2_buildmer_model)
```

The maximal feasible model (MFM) has been developed for us, which has been specified in the `formula` part of our output. In shorthand it translates to this:

`RT ~ 1 + FaceExpression * StoryEmotion + (1 + FaceExpression | Subject) + (1 + FaceExpression | Vignette)`

Our model contains the fixed main effects of `StoryEmotion` *plus* the main effect `FaceExpression` *plus* the interaction between the two factors. For our random effects, we have modeled the main effect of `StoryEmotion` to differ for each of our subjects, and to differ for each of our Vignettes. 

To be able to use our model later, we need to convert it to `lmer` class instead of `buildmer` class. We already know the formula, so let's paste it to `lmer()`. 

```{r}
q2_maximal_model <- lmer(RT ~ 1 + FaceExpression + StoryEmotion + FaceExpression:StoryEmotion + (1 + FaceExpression | Subject) + (1 + FaceExpression | Vignette),
                          data = q2_data_tidied)
```

### Checking Normality Assumptions

We can use the `check_model()` function to check the assumptions of our model.

```{r, warning=FALSE}
check_model(q2_maximal_model)
```

Our model looks pretty good aside from the assessment of normality of residuals, where they seem to drop off at around 1.5 Standard Normal Distribution Quantiles. The 'tail' of our distribution seems to be causing problems. 

We can use the package `{fitdistrplus}` to further investigate the data distribution. The function `descdist()` computes descriptive parameters of our distribution and provides a kewness-kurtosis plot on a Cullen and Frey graph.  

```{r}
descdist(q2_data_tidied$RT)
```

We have lots of different types of data distributions plotted on the Cullen and Frey graph, with our observed data indicated by the blue circle. It appears as though our dataset is quite far away from the theoretical normal distribution is, and is within the Gamma distribution. We can visualise the distribution using a histogram with density along the y-axis, and overlay the plot with a translucent density plot.

```{r, out.width="200%"}
q2_data_tidied  %>% 
  ggplot(aes(x = RT)) +
  geom_histogram(aes(y =..density..),
                 binwidth = 100,
                 colour = "black", fill = "black",
                 alpha = 0.5) +
  geom_density(alpha = .5, fill = "#FF6666") +
  theme_minimal() +
  labs(x = "Reaction Time (ms. )",
       y = "Density",
       title = "Histogram and Density Plot of Reaction Time",
       subtitle = "Our data seems to be more closely\naligned with the Gamma distribution") +
  theme(plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 10), line = 0.5, face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        plot.subtitle = element_text(line = 0.5, hjust = 0.06, margin = margin(b = 10)),
        text = element_text(family = "lato", size = 25))
```

It looks as though we have a skewed normal distribution, which is typical of reaction time data. There is a point below which we don't have many reaction time measures because of physiological constraints. We also have a number of observations of people who have slower reaction times, which produces the gradual gradient of the tail. 

Our next step is to build a model and specify that that our data are of Gamma distribution. We can build this using the `glmer()` function, ensuring that we specify that `family = Gamma`. Gamma mixed models are often much harder to fit, so the random effect structure needs to be much more simplified. Like earlier, we can build the maximal feasible model using the `buildmer()` function.

```{r}
gamma_buildmer_model <- buildmer(RT ~ StoryEmotion * FaceExpression +
                           (1 + StoryEmotion * FaceExpression | Subject) +
                           (1 + StoryEmotion * FaceExpression | Vignette),
                         family = Gamma,
                         data = q2_data_tidied,
                        buildmerControl = buildmerControl(direction = 'order',
                                                          quiet = TRUE))
formula(gamma_buildmer_model)
```

This is problematic - the model `buildmer()` has chosen as the maximal feasible formula is too simplistic:

`RT ~ 1 + FaceExpression + StoryEmotion + StoryEmotion:FaceExpression`.

The maximal model that is actually capable of converging excludes our random slopes. To increase the parameters in our model, we'll need to modify the integer scalar within the `nAGQ` argument. Setting `nAGQ = 0` is less accurate that having it at its default `nAGQ = 1`, but it results in a higher chance of successful convergence [(Source: Stack Exchange)](https://stats.stackexchange.com/questions/544937/when-is-it-appropriate-to-set-nagq-0-in-glmer). 

Let's build a new model with `nAGQ = 0`.

```{r}
gamma_model_nAGQ0 <- buildmer(RT ~ StoryEmotion * FaceExpression +
                           (1 + StoryEmotion * FaceExpression | Subject) +
                           (1 + StoryEmotion * FaceExpression | Vignette),
                        family = Gamma,
                        nAGQ = 0,
                        data = q2_data_tidied,
                        buildmerControl = buildmerControl(direction = 'order',
                                                          quiet = TRUE))
formula(gamma_model_nAGQ0)
```

```{r}
formula(gamma_model_nAGQ0)
```


This MFM is much more inclusive of our parameters. Let's compare our factorial model with normality assumptions against our new gamma model:

```{r}
summary(q2_maximal_model)
```

This is as we predicted earlier in our interaction plot. We have no significant main effects of `StoryEmotion` of `FaceExpression`, but we do have a significant interaction between the two variables. How about in our `gamma_model_nAGQ0`?

```{r}
summary(gamma_model_nAGQ0)
```

This is interesting - the Gamma model has a very different output from the other model. 

## Interpreting the Interaction

We can run pairwise comparisons using the `emmeans()` function. We need to specify our model, what interaction we want to run the t-tests on, and whether we want to change the correction method. By default, `{emmeans}` uses the Tukey correction method, which will suffice for us since only a few of our comparisons are going to be theoretically meaningful. We can, therefore, carry out a Bonferroni adjustment manually by multiplying the calculated *p*-value by the number of theoretically meaningful comparisons.

The `emmeans()` function can't handle our `gamma_model` as it is currently of class `buildmer()`. Let's quickly pass on the MFM that `buildmer()` identified to a `glmer()`.

```{r}
gamma_model_glmer <- glmer(RT ~ 1 + FaceExpression * StoryEmotion + 
        (1 + FaceExpression + StoryEmotion | Subject) + 
        (1 + StoryEmotion * FaceExpression | Vignette),
      data = q2_data_tidied,
      family = Gamma,
      nAGQ = 0)
```


```{r}
emmeans(gamma_model_glmer, pairwise ~ StoryEmotion*FaceExpression, adjust = "none")
```

