---
title: "Assignment 1"
author: '10179889'
date: "09/02/2022"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    font:family: Lato
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

For this assignment, we have been provided with two different datasets. Using the knowledge we have gained from the workshops combined with further reading and wider resources, I will:

-   Wrangle and tidy the data
-   Summarise the data
-   Visualise the data
-   Build and interpret the appropriate models

## Packages

First, let's load in our packages using the `library()` function:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)
library(performance)
library(visdat)
library(emmeans)
library(ggthemes)
library(showtext)
library(buildmer)
```

The `{tidyverse}` package contains a collection of open source R packages that gives us access to a variety of useful functions and commands for data wrangling and visualisation. The `{showtext}` package enables us to load in and use over 1291 fonts from [Google Fonts](https://fonts.google.com/) for our data visualisations, whilst `{ggthemes}` contains extra themes for our plots. The `{afex}`and`{emmeans}` packages will allow us to build ANOVA models and perform post-hoc analyses, respectively. We'll be using the `{afex]` package instead of the `aov()` function in base R as it allows us to build ANOVA models that will work for our experimental designs, as well as using type III sums of squares by default. Finally, `{visdat}` will help us to spot any missing data in our datasets.

To be able to dictate the font family later, we first need to download a font from [Google Fonts](https://fonts.google.com/). I decided to pick the *Lato* Font, as its larger character height lends itself to easier readability at small sizes. Within the `font_add()` command, we can read in our font by specifying what name to assign to our font as well as the path to the font file. Finally, we tell `{showtext}` to automatically render the text:

```{r}
font_add("lato", regular = "Lato-regular.ttf", bold = "Lato-Bold.ttf")
showtext_auto()
```

# Question 1

Let's read in our data and look over the information for question 1:

> <font size = "3"> We have 24 participants in a repeated measures design where we are interested in the effect of context on response time. Our experimental factor (Context) has 3 levels (Neutral vs. Negative vs. Positive). The time it took for participants to respond, which was taken as a measure of reaction time in milliseconds, served as the dependent variable. There were 24 items, and each item appeared in each of the three contexts. </font>

Let's read in our data using the `read_csv()` function. We've organised our files using a .Rproj type to ensure reproducibility. We can also glimpse the first 6 rows of our data using the `head()` function:

```{r, message = FALSE}
q1_data_raw <- read_csv("assignment1_data1.csv")
head(q1_data_raw)
```

## Data Wrangling

Using the `str()` function, we can display the structure of our data.

```{r, message = FALSE}
str(q1_data_raw)
```

As it currently stands, our `subj`, `item`, and `condition` columns are not coded as factors. We can use the `transmute()` function to replace these variables with their factorised equivalents. Within this argument, we can also change the column names to make them more meaningful. We'll pass this on to a new variable, which we'll call `q1_data_tidied`:

```{r}
q1_data_tidied <- q1_data_raw %>% 
  transmute(Subject = factor(subj),
         Item = factor(item),
         Context = factor(condition),
         Response_Time = DV)
head(q1_data_tidied)
```

This is better - Our columns are coded correctly and are labelled meaningfully. Moreover, all data is currently in *long* format; every row corresponds to one observation, which is key for building our models later.

## Data Summarising

Let's create a tibble for the summary statistics of our dataset. We'll first utilise the `group_by()` function to gather our visual-quality variable, then `summarise()` to create a new table consisting of Response Time mean and standard deviation. The data can be arranged from the fastest mean Response Time to the slowest using the `arrange()` function:

```{r}
q1_data_tidied %>% 
  group_by(Context) %>% 
  summarise(mean_RT = mean(Response_Time), sd_RT = sd(Response_Time)) %>%
  arrange(mean_RT)
```

At a glance, it looks as though participants who were presented a items in a negative context had faster response times than those who were presented with items in neutral or postive contexts. Since the output returned no NA results, we know that there is no missing data and therefore no reason to use the `vis_dat()` function to locate the missing data. At this stage, we can create some data visualisations to better present our results.

## Data Visualisations

In this visualisation, we're jittering the data points, meaning that every time we execute the code, the points will jitter to a different position. This implies that our code is not reproducible, because we don't know the seed that R will use to generate the sequence. By setting the seed using the `set.seed()` function, we can ensure that the output will be the same every time it is run. I set the seed to 42 for its reference to Hitch Hikers Guide to the Galaxy...

Next, we can build a visualisation by plotting the raw data points using the `geom_point` function, and the shape of the distribution for each condition using the `geom_violin()` function. We have also added some summary data in the form of the Mean and Confidence Intervals using the `stat_summary()` function. We can dictate the y-axis breaks within the `scale_y_continuous()` function, and change the text font, size, and margin within the `theme()` argument. Finally, we flip the Cartesian coordinates so that the horizontal becomes the vertical and vice versa.

```{r}
q1_data_tidied %>% 
    ggplot(aes(x = Context, y = Response_Time, colour = Context)) +
    geom_violin(width = 0.5) +
    geom_point(alpha = 0.3, position = position_jitter(width = 0.08, seed = 42)) +
    guides(colour = 'none') +
    stat_summary(fun.data = 'mean_cl_boot', colour = 'black') +
    labs(y = "Reaction Time (ms)",
         title = "Effect of Item Context on Reaction Time") +
    scale_y_continuous(breaks = seq(0, 6000, by = 1000),
                       limits = c(0, 6000)) +
    theme_minimal() +
    theme(
      text = element_text(family = "lato", size = 25),
      plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 30), face = "bold"),
      axis.title.x = element_text(size = 30, margin = margin(t = 30)),
      axis.title.y = element_text(size = 30, margin = margin(r = 30))) +
    coord_flip()
```

## Building our Linear Model

```{r}
model <- buildmer(Response_Time ~ Context + 
                    (1 + Context | Item) +
                    (1 + Context | Subject),
                  q1_data_tidied)
```

Response Time is predicted by our fixed effect of condition, plus the random effect of our items, such that we're modelling an individual baseline for each of our items. We're also modelling the random effect of our subjects, such that we're modelling a different intercept for each participant.

```{r}
q1_model <- lmer(Response_Time ~ Context + 
                    (1 + Context | Item) +
                    (1 + Context | Subject),
                  q1_data_tidied)
```

However, when we build this model, we're outputted with the error message `?isSingular`. This tells us that our model that we've build, when considering the number of parameters we're trying to estimate, is more complex than our dataset will allow us to build. In other words, the model that we're trying to build is too complex relative to the richness (or lack of) (poverty??) of our dataset. We can choose to ignore the warning, since it is not a fatal error, especially if we had a strong theoretical reason to model all of the terms in the random effect structure. Alternatively, we could gradually simplify the random effects structure until we resolve the error.

Let's first drop our random slopes:

```{r}
q1_model <- lmer(Response_Time ~ Context +
                   (1 | Subject) +
                   (1 |Item),
                 data = q1_data_tidied)
```

Now, we have a model which does has not generated the warning. Our simpler model has response time predicted by context, whilst modelling the random effects of subjects and items, where each subject and item has its own baseline. However, because we've dropped modelling the random slopes, we're assuming that the difference between the three levels of our experimental condition are the same for all subjects and items. This increases the likelihood of a type 1 error occurring, leading us to think that we have an effect when one is not actually present.

Before we move on, we should check to see whether our model adheres to normality assumptions. To do this, we can use the `check_model()` function found within the `{performance}` package. This will output a number of diagnostic plots, very similar to the assumptions in the context of the linear model, and it'll allow us to quickly tell whether our mixed model seems to be consistent with the assumptions behind mixed models.

```{r}
check_model(q1_model)
```

Although our model seems to break down a little with its normality of residuals, the assumptions seem mostly to have been met ("all models are wrong, but some are useful" - George Box).

We can ask for the output of our model using the `summary()` function.

```{r}
summary(q1_model)
```

More of the random variability can be attributed to our subjects as oppose the the items. Critically, we are interested in our fixed effects. For our dummy coding, *R* using alphabetical information by default when choosing a reference level of our factor against which to compare the other levels of our factor. 

which compares the Intercept (our Negative Context condition) to first the Neutral Context condition, then to the Positive Context condition. The average response times to our Negative condition are 1086.50 ms, which lets us work out the average response times for the other two conditions:

**Neutral Context condition mean:**

```{r}
sum(1086.5 + 98.23)
```

**Positive Context condition mean:**

```{r}
print(1086.5 + 170.8)
```

Our model currently has two comparisons - Negative vs Neutral, and Negative vs Positive conditions. However, one of the issues with using default dummy coding is that there is one comparison that has not yet been conducted - Neutral vs Positive. As it stands, only part of the story has emerged. 

## Likelihood Ratio Test

Let's now compare our experimental model to a model which has dropped the fixed effect of context. First, we need to build this latter model, which we'll call our null model. We'll use the likelihood ratio test (LRT) to assess whether the models are statistically different from each other. It is worth noting that models can only be compared to each other if they are nested - it is essential that our null model is a subset of our full factorial model, and contains the same random effects.

```{r}
q1_model_null <- lmer(Response_Time ~ 
                        (1 | Subject) +
                        (1 | Item),
                    data = q1_data_tidied)
```

We can them compare our two models with the LRT within the `anova()` function:

```{r, warning=FALSE, message=FALSE}
anova(q1_model, q1_model_null)
```

If we have a look at our *p*-value, we can see that our two models differ significantly from each other (*p* < .001. Moreover, by looking at our deviance scores, the residual sums of squares is less for our model with our fixed effects. This tells us that our experimental manipulation seems to be making a difference. Interestingly, the Akaike Information Criteron (AIC), which measures how much 'information' is not captured by our model, is lower in our factorial model (i.e. our fully factorial captures less information). However, it is also worth noting that absolute AIC values can only be interpreted relative to the ACI values from other models.

Following this, we run to run pairwise comparisons where we control for the familywise error rate. We're going to use the `emmenas()` function from the `{emmeans}` package to run our multiple comparisons:

```{r}
emmeans(q1_model, pairwise ~ Context)
```

In this output, we get the estimates for our three experimental conditions, as well as our pairwise comparisons, which compares our condition with each other condition in all possible combinations. By default, the `{emmeans}` function uses the Tukey corrected comparisons method. If we correct for multiple comparisons, we see that only one pairwise comparison is significant - Negative vs Positive condition. The Neutral vs Positive conditions, which was significant in the model estimates earlier, is no longer significant when taking into account multiple comparisons. 

## Conclusion
In our model, where our experimental factor

Post hoc comparisons applying Tukey correction confirmed that words appearing in Negative contexts resulted in significantly faster response times than words appearing in Positive contexts (*p* = .013, d = .), but not Neutral contexts (*p* = .145, d = ). The difference in response times given for words in Neutral contexts and Positive Contexts was not statistically significant (*p* = .312, d = ).

By using a linear mixed models, we were able to model the individual variation due to participants and items, and build up a model which is able to take on board all of the information in our dataset. By working over the raw data points themselves, the linear mixed model approach uses a lot more information that is in the dataset. Finally, we can compare different models to each other using the Likelihood Ratio Test.

# Question 2

> <font size = "3"> In a 2 x 2 repeated measures experiment, participants had to respond to a face that depicted either “Anger” or “Fear”. Each face was presented after a Story Vignette that described either an angry or fearful situation. There were 32 participants and 32 vignettes. Two independent variables were manipulated within-participants in a fully factorial design; story emotion with two levels (Anger and Fear) and Face Expression with two levels (Anger and Fear). The time it took for participants to respond to the face, which was taken as a measure of reaction time, served as the dependent variable. </font>

