---
title: "Assignment 1"
author: '10179889'
date: "09/02/2022"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float: yes
    font:family: Lato
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

For this assignment, we have been provided with two different datasets. Using the knowledge we have gained from the workshops combined with further reading and wider resources, I will:

-   Wrangle and tidy the data
-   Summarise the data
-   Visualise the data
-   Build and interpret the appropriate models

## Packages

First, let's load in our packages using the `library()` function:

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)
library(performance)
library(visdat)
library(emmeans)
library(ggthemes)
library(showtext)
library(buildmer)
```

The `{tidyverse}` package contains a collection of open source R packages that gives us access to a variety of useful functions and commands for data wrangling and visualisation. The `{showtext}` package enables us to load in and use over 1291 fonts from [Google Fonts](https://fonts.google.com/) for our data visualisations, whilst `{ggthemes}` contains extra themes for our plots. The `{afex}`and`{emmeans}` packages will allow us to build ANOVA models and perform post-hoc analyses, respectively. We'll be using the `{afex]` package instead of the `aov()` function in base R as it allows us to build ANOVA models that will work for our experimental designs, as well as using type III sums of squares by default. Finally, `{visdat}` will help us to spot any missing data in our datasets.

To be able to dictate the font family later, we first need to download a font from [Google Fonts](https://fonts.google.com/). I decided to pick the *Lato* Font, as its larger character height lends itself to easier readability at small sizes. Within the `font_add()` command, we can read in our font by specifying what name to assign to our font as well as the path to the font file. Finally, we tell `{showtext}` to automatically render the text:

```{r}
font_add("lato", regular = "Lato-regular.ttf", bold = "Lato-Bold.ttf")
showtext_auto()
```

# Question 1

Let's read in our data and look over the information for question 1:

> <font size = "3"> We have 24 participants in a repeated measures design where we are interested in the effect of context on response time. Our experimental factor (Context) has 3 levels (Neutral vs. Negative vs. Positive). The time it took for participants to respond, which was taken as a measure of reaction time in milliseconds, served as the dependent variable. There were 24 items, and each item appeared in each of the three contexts. </font>

Let's read in our data using the `read_csv()` function. We've organised our files using a .Rproj type to ensure reproducibility. We can also glimpse the first 6 rows of our data using the `head()` function:

```{r, message = FALSE}
q1_data_raw <- read_csv("assignment1_data1.csv")
head(q1_data_raw)
```

## Data Wrangling

Using the `str()` function, we can display the structure of our data.

```{r, message = FALSE}
str(q1_data_raw)
```

As it currently stands, our `subj`, `item`, and `condition` columns are not coded as factors. We can use the `transmute()` function to replace these variables with their factorised equivalents. Within this argument, we can also change the column names to make them more meaningful. We'll pass this on to a new variable, which we'll call `q1_data_tidied`:

```{r}
q1_data_tidied <- q1_data_raw %>% 
  transmute(Subject = factor(subj),
         Item = factor(item),
         Context = factor(condition),
         Response_Time = DV)
head(q1_data_tidied)
```

This is better - Our columns are coded correctly and are labelled meaningfully. Moreover, all data is currently in *long* format; every row corresponds to one observation, which is key for building our models later.

## Data Summarising

Let's create a tibble for the summary statistics of our dataset. We'll first utilise the `group_by()` function to gather our visual-quality variable, then `summarise()` to create a new table consisting of Response Time mean and standard deviation. The data can be arranged from the fastest mean Response Time to the slowest using the `arrange()` function:

```{r}
q1_data_tidied %>% 
  group_by(Context) %>% 
  summarise(mean_RT = mean(Response_Time), sd_RT = sd(Response_Time)) %>%
  arrange(mean_RT)
```

At a glance, it looks as though participants who were presented a items in a negative context had faster response times than those who were presented with items in neutral or postive contexts. Since the output returned no NA results, we know that there is no missing data and therefore no reason to use the `vis_dat()` function to locate the missing data. At this stage, we can create some data visualisations to better present our results.

## Data Visualisations

In this visualisation, we're jittering the data points, meaning that every time we execute the code, the points will jitter to a different position. This implies that our code is not reproducible, because we don't know the seed that R will use to generate the sequence. By setting the seed using the `set.seed()` function, we can ensure that the output will be the same every time it is run. I set the seed to 42 for its reference to Hitch Hikers Guide to the Galaxy...

Next, we can build a visualisation by plotting the raw data points using the `geom_point` function, and the shape of the distribution for each condition using the `geom_violin()` function. We have also added some summary data in the form of the Mean and Confidence Intervals using the `stat_summary()` function. We can dictate the y-axis breaks within the `scale_y_continuous()` function, and change the text font, size, and margin within the `theme()` argument. Finally, we flip the Cartesian coordinates so that the horizontal becomes the vertical and vice versa.

```{r, out.width="180%", out.height="180%"}
q1_data_tidied %>% 
    ggplot(aes(x = Context, y = Response_Time, colour = Context)) +
    geom_violin(width = 0.5) +
    geom_point(alpha = 0.3, position = position_jitter(width = 0.08, seed = 42)) +
    guides(colour = 'none') +
    stat_summary(fun.data = 'mean_cl_boot', colour = 'black') +
    labs(y = "Reaction Time (ms)",
         title = "Effect of Item Context on Reaction Time") +
    scale_y_continuous(breaks = seq(0, 6000, by = 1000),
                       limits = c(0, 6000)) +
    theme_minimal() +
    theme(
      text = element_text(family = "lato", size = 25),
      plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 30), face = "bold"),
      axis.title.x = element_text(size = 30, margin = margin(t = 30)),
      axis.title.y = element_text(size = 30, margin = margin(r = 30)),
      plot.margin = unit(rep(1.2, 4), "cm")) +
    coord_flip()
```

<p>&nbsp;</p>

## Building our Linear Model

```{r}
model <- buildmer(Response_Time ~ Context + 
                    (1 + Context | Item) +
                    (1 + Context | Subject),
                  q1_data_tidied)
```

Response Time is predicted by our fixed effect of condition, plus the random effect of our items, such that we're modelling an individual baseline for each of our items. We're also modelling the random effect of our subjects, such that we're modelling a different intercept for each participant.

```{r}
q1_model <- lmer(Response_Time ~ Context + 
                    (1 + Context | Item) +
                    (1 + Context | Subject),
                  q1_data_tidied)
```

However, when we build this model, we're outputted with the error message `?isSingular`. This tells us that our model that we've build, when considering the number of parameters we're trying to estimate, is more complex than our dataset will allow us to build. In other words, the model that we're trying to build is too complex relative to the richness (or lack of) (poverty??) of our dataset. We can choose to ignore the warning, since it is not a fatal error, especially if we had a strong theoretical reason to model all of the terms in the random effect structure. Alternatively, we could gradually simplify the random effects structure until we resolve the error.

Let's first drop our random slopes:

```{r}
q1_model <- lmer(Response_Time ~ Context +
                   (1 | Subject) +
                   (1 |Item),
                 data = q1_data_tidied)
```

Now, we have a model which does has not generated the warning. Our simpler model has response time predicted by context, whilst modelling the random effects of subjects and items, where each subject and item has its own baseline. However, because we've dropped modelling the random slopes, we're assuming that the difference between the three levels of our experimental condition are the same for all subjects and items. This increases the likelihood of a type 1 error occurring, leading us to think that we have an effect when one is not actually present.

Before we move on, we should check to see whether our model adheres to normality assumptions. To do this, we can use the `check_model()` function found within the `{performance}` package. This will output a number of diagnostic plots, very similar to the assumptions in the context of the linear model, and it'll allow us to quickly tell whether our mixed model seems to be consistent with the assumptions behind mixed models.

```{r}
check_model(q1_model)
```

Although our model seems to break down a little with its normality of residuals, the assumptions seem mostly to have been met ("all models are wrong, but some are useful" - George Box).

We can ask for the output of our model using the `summary()` function.

```{r}
summary(q1_model)
```

More of the random variability can be attributed to our subjects as oppose the the items. Critically, we are interested in our fixed effects. For our dummy coding, *R* using alphabetical information by default when choosing a reference level of our factor against which to compare the other levels of our factor.

which compares the Intercept (our Negative Context condition) to first the Neutral Context condition, then to the Positive Context condition. The average response times to our Negative condition are 1086.50 ms, which lets us work out the average response times for the other two conditions:

**Neutral Context condition mean:**

```{r}
sum(1086.5 + 98.23)
```

**Positive Context condition mean:**

```{r}
print(1086.5 + 170.8)
```

Our model currently has two comparisons - Negative vs Neutral, and Negative vs Positive conditions. However, one of the issues with using default dummy coding is that there is one comparison that has not yet been conducted - Neutral vs Positive. As it stands, only part of the story has emerged.

### Likelihood Ratio Test

Let's now compare our experimental model to a model which has dropped the fixed effect of context. First, we need to build this latter model, which we'll call our null model. We'll use the likelihood ratio test (LRT) to assess whether the models are statistically different from each other. It is worth noting that models can only be compared to each other if they are nested - it is essential that our null model is a subset of our full factorial model, and contains the same random effects.

```{r}
q1_model_null <- lmer(Response_Time ~ 
                        (1 | Subject) +
                        (1 | Item),
                    data = q1_data_tidied)
```

We can them compare our two models with the LRT within the `anova()` function:

```{r, warning=FALSE, message=FALSE}
anova(q1_model, q1_model_null)
```

If we have a look at our *p*-value, we can see that our two models differ significantly from each other (*p* \< .001. Moreover, by looking at our deviance scores, the residual sums of squares is less for our model with our fixed effects. This tells us that our experimental manipulation seems to be making a difference. Interestingly, the Akaike Information Criteron (AIC), which measures how much 'information' is not captured by our model, is lower in our factorial model (i.e. our fully factorial captures less information). However, it is also worth noting that absolute AIC values can only be interpreted relative to the ACI values from other models.

Following this, we run to run pairwise comparisons where we control for the familywise error rate. We're going to use the `emmenas()` function from the `{emmeans}` package to run our multiple comparisons:

```{r}
emmeans(q1_model, pairwise ~ Context)
```

In this output, we get the estimates for our three experimental conditions, as well as our pairwise comparisons, which compares our condition with each other condition in all possible combinations. By default, the `{emmeans}` function uses the Tukey corrected comparisons method. If we correct for multiple comparisons, we see that only one pairwise comparison is significant - Negative vs Positive condition. The Neutral vs Positive conditions, which was significant in the model estimates earlier, is no longer significant when taking into account multiple comparisons.

## Conclusion

In our model, where our experimental factor

Post hoc comparisons applying Tukey correction confirmed that words appearing in Negative contexts resulted in significantly faster response times than words appearing in Positive contexts (*p* = .013, d = .), but not Neutral contexts (*p* = .145, d = ). The difference in response times given for words in Neutral contexts and Positive Contexts was not statistically significant (*p* = .312, d = ).

By using a linear mixed models, we were able to model the individual variation due to participants and items, and build up a model which is able to take on board all of the information in our dataset. By working over the raw data points themselves, the linear mixed model approach uses a lot more information that is in the dataset. Finally, we can compare different models to each other using the Likelihood Ratio Test.

# Question 2

> <font size = "3"> In a 2 x 2 repeated measures experiment, participants had to respond to a face that depicted either "Anger" or "Fear". Each face was presented after a Story Vignette that described either an angry or fearful situation. There were 32 participants and 32 vignettes. Two independent variables were manipulated within-participants in a fully factorial design; story emotion with two levels (Anger and Fear) and Face Expression with two levels (Anger and Fear). The time it took for participants to respond to the face, which was taken as a measure of reaction time, served as the dependent variable. </font>

Let's read in our data using the `{tidyverse}` function `read_csv()`:

```{r}
q2_data_raw <- read_csv("assignment1_data2.csv")
head(q2_data_raw)
```

## Data Wrangling

Like the previous question, we need to code all our columns (bar our DV) as factors. The column titles are labelled meaningfully enough, so no need to change them.

```{r}
q2_data_tidied <- q2_data_raw %>% 
  mutate(Subject = factor(Subject),
            Vignette = factor(Vignette),
            StoryEmotion = factor(StoryEmotion),
            FaceExpression = factor(FaceExpression))
head(q2_data_tidied)
```

This is much better. We can use the `str()` function to explore our tibble in a bit more detail:

```{r}
str(q2_data_tidied)
```

We can see that `StoryEmotion` and `FaceExpression` have the two levels coded in.

## Data Summarising

Let's generate some descriptive statistics to get an idea of what's going on. We need to group by both of our independent variables using the `group_by()` function, then ask *R* to work out the mean and standard deviation of reaction time within the `summarise()` function. Like before, we can arrange the mean values in ascending order to see which group had the fastest reaction times. I've mapped the outcome onto a new variable for later use in an interaction plot.

```{r}
q2_descriptives <- q2_data_tidied %>% 
  group_by(StoryEmotion, FaceExpression) %>% 
  summarise(mean = mean(RT), sd = sd(RT)) %>% 
  arrange(mean)
head(q2_descriptives)
```

Like the previous question, our output hasn't produced any NA results, so no need to use the `{visdat}` function to identify the location of missing data. At face value, it looks like participants were quickest at identifying the facial expressions when the story preceding it was consistent with the face emotion. Let's generate a visualisation to get a clearer idea.

## Data Visualisation

```{r}
q2_data_tidied %>% 
  ggplot(aes(x = StoryEmotion:FaceExpression, y = RT, colour = FaceExpression)) +
  geom_violin() +
  geom_point(alpha = 0.2, position = position_jitter(width = 0.1, seed = 42)) +
  stat_summary(fun.data = 'mean_cl_boot', colour = 'black') +
  theme_minimal() +
  scale_y_continuous(breaks = seq(0, 10000, by = 1250),
                     limits = c(0, 10000)) +
  scale_x_discrete(labels = c("Anger:Anger" = "Anger",
                              "Anger:Fear" = "Anger",
                              "Fear:Anger" = "Fear",
                              "Fear:Fear" = "Fear")) +
  labs(x = "Story Emotion",
       y = "Reaction Time (ms. )",
       title = "Examining the Effect of Story Emotion\n and Face Expression on Reaction Time") +
  theme(plot.title = element_text(size = 40, hjust = 0.5, margin = margin(b = 25), line = 0.5, face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        text = element_text(family = "lato", size = 25)) +
  coord_flip()
```

<p>&nbsp;</p>

We can also build an interaction plot to get an idea of any interactions that may be present. First, we'll bring the legend closer to the lines by creating a new dataset that includes the y-values of the points where both lines end. We also need the labels that will be at the end of the lines. For this we use the function `case_when()`.

```{r}
labels <- q2_descriptives %>% 
  filter(StoryEmotion == "Fear") %>% 
  mutate(label = case_when(FaceExpression == "Anger" ~ "Angry Face\nExpression",
                           FaceExpression == "Fear" ~ "Fearful Face\nExpressions"))
head(labels)
```

Using the `descriptive_stats` dataset we created earlier, we can add the geom_text to our interaction plot and pass our dataset `labels` to the geom. Among the aesthetics, `geom_text()` includes the aesthetic label, which stands for the text we will add to the visualization. We've also scaled down the y-axis using the `scale_y_continuous()` function to visually increase the distance between the means, aiding interpretation.

```{r}
q2_descriptives %>% 
  ggplot(aes(x = StoryEmotion, y = mean)) +
  geom_line(size = 1.2, aes(group = FaceExpression, colour = FaceExpression)) +
  geom_point(size = 2.6, aes(colour = FaceExpression), shape = 15) +
  geom_text(size = 8, aes(label = label,
                          colour = FaceExpression),
            data = labels,
            nudge_x = 0.17,
            nudge_y = 70,
            lineheight = 0.5) +
  guides(colour = 'none') +
  scale_y_continuous(breaks = seq(1800, 2800, by = 200),
                     limits = c(1800, 2800)) +
  labs(x = "Story Emotion",
       y = "Reaction Time (ms. )",
       title = "Examining the Interaction Between\nStory Emotion and Face Expression") +
  theme_hc() +
  theme(plot.title = element_text(size = 40, hjust = 0.5, line = 0.5, margin = margin(b = 25), face = "bold"),
        axis.title.x = element_text(size = 30, margin = margin(t = 20)),
        axis.title.y = element_text(size = 30, margin = margin(r = 20)),
        text = element_text(family = "lato", size = 25)
        )
```

<p>&nbsp;</p>

We can see here that we have a crossover interaction as the polarity of the difference flips. The graph shows that Reaction Time was faster for Angry Face Expressions when the preceding story was also angry. Likewise, Reaction Time was faster for Fearful Face Expressions when the preceding story was also Fearful. Due to our crossover interaction, it is unlikely that we'll have significant main effects of *both* Story Emotion and Face Expression variables, but a significant interaction effect is likely.

## Building our Mixed-Model

### Setting up our Contrasts

*R* assigns Treatment Contrasts to factors and orders their levels alphabetically. This default mapping will only be correct for a given data=set if the levels' alphabetical ordering matches our desired contrast coding. We're going to use sum contrast coding, where we want to end up with a mixed-model with the intercept equal to the grand mean of our experimental conditions. This makes it much easier to estimate the parameters in our model, and also ensures that we're interpreting main effects instead of simple effects. 

When we're building models for factorial data in *R*, by default the factors are coded using dummy coding. This is problematic for models with interactions, as it makes it harder for us to interpret our parameter estimates, and critically, can result in misinterpretation of main vs simple effects. 

```{r}
contrasts(q2_data_tidied$StoryEmotion) <- contr.sum(2)
contrasts(q2_data_tidied$FaceExpression) <- contr.sum(2)
```

### Building our Maximal Model

Let's first create a maximal model where we attempt to model the most complex random effects structure - `StoryEmotion + FaceExpression * StoryEmotion : FaceExpression.` We're attempting to model our random effects such that the main effects *and* interaction effects differ for each of our participants and each vignettes. 

```{r}
q2_maximal_model <- lmer(RT ~ StoryEmotion * FaceExpression +
                   (1 + StoryEmotion * FaceExpression | Subject) +
                   (1 + StoryEmotion * FaceExpression | Vignette),
                 data = q2_data_tidied)
```

When we try to build our maximal model, we receive a warning message telling us that it was unable to converge on a solution of a model that fits our dataset. In other words, our model is too complicated in relation to our data-set. To remedy this issue, we would normally need to simplify our random effect structure until we reach a model which fits our data-set with the appropriate parameters. This would mean continually building models in a backwards direction until we found a model which didn't spit out the `Model failed to converge...` warning message. As well as being time-consuming, this method is not guaranteed to produce the maximal feasible model.  

We can use the `{buildmer}` package to remedy this dilemma. The package consists of a number of functions designed to fit specific types of models. By piggybacking off the `lmer()` function from `{lme4}`, the  `buildmer()` function fits our mixed-effect model to find the maximal feasible model. The good thing about this package is that it uses the exact syntax as building a model in `{lme4}`. 

```{r, message=FALSE}
q2_max_feas_model <- buildmer(RT ~ StoryEmotion * FaceExpression +
                          (1 + StoryEmotion * FaceExpression | Subject) +
                          (1 + StoryEmotion * FaceExpression | Vignette),
                        data = q2_data_tidied)
```

The maximal feasible model has been developed for us, which we can inspect using the `summary()` function:

```{r}
summary(q2_max_feas_model)
```

At the beginning of this output, we are given the model formula:

`RT ~ 1 + FaceExpression + StoryEmotion + FaceExpression:StoryEmotion + (1 + FaceExpression | Subject) + (1 + FaceExpression | Vignette)`

Our model contains the fixed main effects of `StoryEmotion` *plus* the main effect `FaceExpression` *plus* the interaction between the two factors. For our random effects, we have modeled the main effect of `StoryEmotion` to differ for each of our subjects, and to differ for each of our Vignettes. 

### Checking Normality Assumptions

